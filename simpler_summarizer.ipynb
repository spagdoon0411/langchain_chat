{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Tuple, Literal\n",
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMessageSource(BaseModel):\n",
    "    \"\"\"Base class representing a source of information for a message.\"\"\"\n",
    "\n",
    "    sourceId: str\n",
    "    messageId: str\n",
    "    pageContent: str\n",
    "    metadata: dict\n",
    "    createdAt: int\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    \"\"\"Base class representing a chat message.\"\"\"\n",
    "\n",
    "    messageId: str\n",
    "    messageType: str\n",
    "    userId: str\n",
    "    chatId: str\n",
    "    content: str\n",
    "    createdAt: int\n",
    "    sources: Optional[List[ChatMessageSource]] = None\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    \"\"\"Base class representing a chat message.\"\"\"\n",
    "\n",
    "    messageId: str\n",
    "    messageType: str\n",
    "    userId: str\n",
    "    chatId: str\n",
    "    content: str\n",
    "    createdAt: int\n",
    "    sources: Optional[List[ChatMessageSource]] = None\n",
    "\n",
    "\n",
    "class ModelKwargs(BaseModel):\n",
    "    maxTokens: Optional[int] = None\n",
    "    temperature: Optional[float] = None\n",
    "    topP: Optional[float] = None\n",
    "    stopSequences: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "# Define the ModelProvider type\n",
    "ModelProvider = Literal[\"sagemaker\", \"bedrock\"]\n",
    "\n",
    "\n",
    "class ModelKwargs(BaseModel):\n",
    "    maxTokens: Optional[int] = None\n",
    "    temperature: Optional[float] = None\n",
    "    topP: Optional[float] = None\n",
    "    stopSequences: Optional[list[str]] = None\n",
    "\n",
    "\n",
    "class ModelBase(BaseModel):\n",
    "    provider: ModelProvider\n",
    "    modelId: str\n",
    "    region: Optional[str] = None\n",
    "\n",
    "\n",
    "class LLMModelBase(ModelBase):\n",
    "    modelKwargs: Optional[ModelKwargs] = None\n",
    "\n",
    "\n",
    "class BedRockLLMModel(LLMModelBase):\n",
    "    provider: Literal[\"bedrock\"]\n",
    "\n",
    "\n",
    "class HandoffConfig(BedRockLLMModel):\n",
    "    details: Optional[list[str]] = None\n",
    "    windowSize: Optional[int] = None\n",
    "    windowOverlap: Optional[int] = None\n",
    "\n",
    "\n",
    "# interface HandoffConfig extends BedRockLLMModel {\n",
    "#     details?: string[];\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "from langchain_core.messages.utils import get_buffer_string\n",
    "from typing import Optional, Iterator\n",
    "import boto3\n",
    "\n",
    "FAILED_TO_SUMMARIZE = \"Summarizer failed to generate a response.\"\n",
    "\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "DEFAULT_TEMPERATURE = 0.1\n",
    "DEFAULT_TOP_P = 0.90\n",
    "DEFAULT_STOP_SEQUENCES = []\n",
    "\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(self, handoff_config: HandoffConfig):\n",
    "        self.handoff_config = handoff_config\n",
    "        self.bedrock = boto3.client(\"bedrock-runtime\")\n",
    "        self.model_id = handoff_config.modelId\n",
    "\n",
    "        # A model prompt consists of a role definition, a prompt describing the summarization task,\n",
    "        # a conversation, and a tail prompt (which includes a list of types of details to focus on)\n",
    "\n",
    "        self.role_definition = str(\n",
    "            \"You are a detailed note-taker for a customer service chatbot that helps \"\n",
    "            \"users solve technical issues. You carefully read through conversations \"\n",
    "            \"and focus on the details you're asked to find by the system. Do not \"\n",
    "            \"output anything except text.\"\n",
    "        )\n",
    "\n",
    "        # For extending a summary\n",
    "        self.recursive_prompt = lambda summary: (\n",
    "            f\"This is a summary of the conversation so far:\\n\\n{summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages below. \"\n",
    "            \"Be sure to return a summary of the whole conversation, not just the \"\n",
    "            \"new messages. Use bullet points, being sure to add new bullet points to \"\n",
    "            \"reflect these new messages. Feel free to modify the summary to make it more \"\n",
    "            \"concise, but make sure to keep all the important details.\",\n",
    "        )\n",
    "\n",
    "        # For creating a new summary\n",
    "        self.base_prompt: str = str(\n",
    "            \"Create a summary of the conversation below that captures the key \"\n",
    "            \"points of the conversation. Keep the summary to a few bullet points.\"\n",
    "        )\n",
    "\n",
    "        # Separates instructions from conversation in the prompt\n",
    "        self.conv_start_delim = \"CONVERSATION START:\"\n",
    "        self.conv_end_delim = \"CONVERSATION END\"\n",
    "\n",
    "        # Create a non-empty tail prompt if config provides details to focus on\n",
    "        self.types_of_details = self._details_string(handoff_config.details)\n",
    "        self.tail_prompt: str = (\n",
    "            (\n",
    "                \"Especially focus on the following types of details:\\n\"\n",
    "                f\"{self.types_of_details}\\n\"\n",
    "                \"...as well as any other details that are important to the purpose of the conversation.\"\n",
    "            )\n",
    "            if handoff_config.details\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    def _details_string(self, details: list[str]) -> str:\n",
    "        return \"\\n\".join([f\"- {detail}\" for detail in details])\n",
    "\n",
    "    def _conversator_name(self, message_type: Literal[\"ai\", \"human\"]) -> str:\n",
    "        match message_type:\n",
    "            case \"ai\":\n",
    "                return \"Chatbot\"\n",
    "            case \"human\":\n",
    "                return \"Human\"\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown message type: {message_type}\")\n",
    "\n",
    "    def _create_conversation_string(self, messages: Iterator[ChatMessage]) -> str:\n",
    "        return \"\\n\\n\".join(\n",
    "            [f\"{self._conversator_name(m.messageType)}: {m.content}\" for m in messages]\n",
    "        )\n",
    "\n",
    "    def _create_summarization_prompt(\n",
    "        self,\n",
    "        messages: Iterator[ChatMessage],\n",
    "        existing_summary=None,\n",
    "        supports_system_prompt=False,\n",
    "    ) -> dict:\n",
    "\n",
    "        if existing_summary:\n",
    "            task_prompt = self.recursive_prompt(existing_summary)\n",
    "        else:\n",
    "            task_prompt = self.base_prompt\n",
    "\n",
    "        conversation = self._create_conversation_string(messages)\n",
    "\n",
    "        complete_prompt_components = [\n",
    "            task_prompt,\n",
    "            self.conv_start_delim,\n",
    "            conversation,\n",
    "            self.conv_end_delim,\n",
    "            self.tail_prompt,\n",
    "        ]\n",
    "\n",
    "        if supports_system_prompt:\n",
    "            # Add the role definition to the system prompts\n",
    "            complete_prompt = \"\\n\\n\".join(complete_prompt_components)\n",
    "            return {\"prompt\": complete_prompt, \"system_prompts\": [self.role_definition]}\n",
    "        else:\n",
    "            # Add the role definition to the user prompt\n",
    "            complete_prompt = \"\\n\\n\".join(\n",
    "                [self.role_definition] + complete_prompt_components\n",
    "            )\n",
    "            return {\"prompt\": complete_prompt}\n",
    "\n",
    "    def _non_text_response_types(self, response: dict) -> set:\n",
    "        response_content = response[\"output\"][\"message\"][\"content\"]\n",
    "        content_types = set()\n",
    "        for content in response_content:\n",
    "            content_types.update(content.keys())\n",
    "        return content_types - {\"text\"}\n",
    "\n",
    "    def summarize(self, francis_messages: Iterator[ChatMessage]) -> Optional[str]:\n",
    "        prompt = self._create_summarization_prompt(francis_messages)\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt[\"prompt\"]}]}]\n",
    "        system_prompts = prompt.get(\"system_prompts\", None)\n",
    "\n",
    "        inference_config = {\n",
    "            \"maxTokens\": self.handoff_config.modelKwargs.maxTokens\n",
    "            or DEFAULT_MAX_TOKENS,\n",
    "            \"temperature\": self.handoff_config.modelKwargs.temperature\n",
    "            or DEFAULT_TEMPERATURE,\n",
    "            \"topP\": self.handoff_config.modelKwargs.topP or DEFAULT_TOP_P,\n",
    "            \"stopSequences\": self.handoff_config.modelKwargs.stopSequences\n",
    "            or DEFAULT_STOP_SEQUENCES,\n",
    "        } | (\n",
    "            {\"region\": self.handoff_config.region} if self.handoff_config.region else {}\n",
    "        )\n",
    "\n",
    "        converse_kwargs = {\n",
    "            \"modelId\": self.handoff_config.modelId,\n",
    "            \"messages\": messages,\n",
    "            \"inferenceConfig\": inference_config,\n",
    "        } | ({\"systemPrompts\": system_prompts} if system_prompts else {})\n",
    "        # Some models support system prompts; some do not\n",
    "\n",
    "        try:\n",
    "            response = self.bedrock.converse(**converse_kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while summarizing messages: {e}\")\n",
    "            return FAILED_TO_SUMMARIZE\n",
    "\n",
    "        if response.get(\"stopReason\") not in [\n",
    "            \"end_turn\",\n",
    "            \"stop_sequence\",\n",
    "            \"max_tokens\",\n",
    "        ]:\n",
    "            print(\n",
    "                f\"Unexpected stop reason from model {self.model_id}: {response.get('stop_reason')}\"\n",
    "            )\n",
    "            return FAILED_TO_SUMMARIZE\n",
    "\n",
    "        if non_text_types := self._non_text_response_types(response):\n",
    "            # TODO: log this\n",
    "            print(\n",
    "                f\"Unexpected response mode; did not expect non-text content: {non_text_types}\"\n",
    "            )\n",
    "\n",
    "        # Aggregate all text outputs from the response\n",
    "        response_content = response[\"output\"][\"message\"][\"content\"]\n",
    "        text_outputs = [\n",
    "            content.get(\"text\") for content in response_content if \"text\" in content\n",
    "        ]\n",
    "\n",
    "        return \"\\n\\n\".join(text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"GSI1\"\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "TABLE_NAME = \"FrancisChatbotStack-nasrullah-dev-ConversationStoreConversationTable631357AC-CI2U1TKWQOEW\"\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "\n",
    "def get_chat_messages_by_time_key(\n",
    "    user_id: str, chat_id: str, timestamp: str = \"\"\n",
    ") -> dict:\n",
    "    return {\n",
    "        \"GSI1PK\": f\"{user_id}#CHAT#{chat_id}\",\n",
    "        \"GSI1SK\": timestamp,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_next_token(next_token: str) -> dict | None:\n",
    "    parts = next_token.split(\"|\")\n",
    "    if len(parts) != 4:\n",
    "        print(f\"Invalid next_token format: {next_token}\")\n",
    "        return None\n",
    "\n",
    "    PK, SK, GSI1PK, GSI1SK = parts\n",
    "    return {\n",
    "        \"PK\": PK,\n",
    "        \"SK\": SK,\n",
    "        \"GSI1PK\": GSI1PK,\n",
    "        \"GSI1SK\": GSI1SK if GSI1SK else None,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_next_token(params: dict) -> str:\n",
    "    return (\n",
    "        f\"{params['PK']}|{params['SK']}|{params['GSI1PK']}|{params.get('GSI1SK', '')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def list_chat_messages(\n",
    "    user_id: str,\n",
    "    chat_id: str,\n",
    "    next_token: Optional[str] = None,\n",
    "    limit: int = 50,\n",
    "    ascending: bool = True,\n",
    ") -> Tuple[List[ChatMessage], Optional[str]]:\n",
    "    keys = get_chat_messages_by_time_key(user_id, chat_id, \"\")\n",
    "\n",
    "    exclusive_start_key = {}\n",
    "    if isinstance(next_token, str):\n",
    "        parsed_token = parse_next_token(next_token)\n",
    "        exclusive_start_key = (\n",
    "            {\"ExclusiveStartKey\": parsed_token} if parsed_token else {}\n",
    "        )\n",
    "\n",
    "    query_input = {\n",
    "        \"IndexName\": index_name,\n",
    "        \"KeyConditionExpression\": \"GSI1PK = :PK\",\n",
    "        \"ExpressionAttributeValues\": {\":PK\": keys[\"GSI1PK\"]},\n",
    "        \"Limit\": limit,\n",
    "        \"ScanIndexForward\": ascending,\n",
    "        **exclusive_start_key,\n",
    "    }\n",
    "\n",
    "    response = table.query(**query_input)\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            chatId=record[\"chatId\"],\n",
    "            userId=record[\"userId\"],\n",
    "            messageId=record[\"messageId\"],\n",
    "            content=record[\"data\"][\"content\"],\n",
    "            createdAt=int(record[\"createdAt\"]),\n",
    "            messageType=record[\"messageType\"],\n",
    "        )\n",
    "        for record in response.get(\"Items\", [])\n",
    "    ]\n",
    "\n",
    "    next_new_token = None\n",
    "    if response.get(\"LastEvaluatedKey\"):\n",
    "        next_new_token = generate_next_token(response[\"LastEvaluatedKey\"])\n",
    "\n",
    "    return messages, next_new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = [\n",
    "    \"The user's primary issue\",\n",
    "    \"Questions that the user asked\",\n",
    "    \"Places where the user seemed confused\",\n",
    "    \"Places where the user got stuck\",\n",
    "    \"Recommendations the AI made\",\n",
    "    \"Solutions that did or didn't work\",\n",
    "    \"Places where the user seemed frustrated or wanted to talk to a human\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# MODEL = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "handoff_config = HandoffConfig(\n",
    "    provider=\"bedrock\",\n",
    "    region=None,\n",
    "    modelId=MODEL,\n",
    "    modelKwargs=ModelKwargs(\n",
    "        temperature=0.1,\n",
    "        maxTokens=1024,\n",
    "        topP=0.90,\n",
    "    ),\n",
    "    details=details,\n",
    ")\n",
    "summarizer = Summarizer(handoff_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usda\n",
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"AWS_PROFILE\"] = \"usda\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "!echo $AWS_PROFILE\n",
    "!echo $AWS_DEFAULT_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(messageId='819fa771-b117-443c-b55f-9a3aaaefd077', messageType='human', userId='9811f3a0-d0d1-701b-29e4-f7a680d50e85', chatId='9112a4c0-6a2e-4a56-9280-5e25c95f8888', content='hello\\n', createdAt=1737745583521, sources=None),\n",
       " ChatMessage(messageId='254788aa-9114-470c-be8e-dac47d63c1dc', messageType='ai', userId='9811f3a0-d0d1-701b-29e4-f7a680d50e85', chatId='9112a4c0-6a2e-4a56-9280-5e25c95f8888', content=\"Hello! I'm the Cal Poly IT Help Desk assistant. How can I help you today?\", createdAt=1737745583567, sources=None),\n",
       " ChatMessage(messageId='eb7cf1f8-9f3c-49ea-a5e0-95d6273d5053', messageType='human', userId='9811f3a0-d0d1-701b-29e4-f7a680d50e85', chatId='9112a4c0-6a2e-4a56-9280-5e25c95f8888', content='how do I change my display name?', createdAt=1737745609388, sources=None),\n",
       " ChatMessage(messageId='5f340136-3dc0-48c8-a4d8-0f51e6e94d04', messageType='ai', userId='9811f3a0-d0d1-701b-29e4-f7a680d50e85', chatId='9112a4c0-6a2e-4a56-9280-5e25c95f8888', content='Here are the steps to change your display name when sending email from Outlook:\\n\\n1. Log in to the My Cal Poly Portal > Personal Info tab.\\n2. Under Personal Info > My Info, by the Preferred Name field click the Edit link.\\n3. Click the Continue link, then:\\n   a. Employees: Name Type > Preferred, click the Edit button.\\n   b. Students: Personal Info > Names, click the Edit button.\\n4. Enter your name using upper and lower case letters. Do not include periods. Click the Save button, then the OK button.\\n\\nNote: It may take up to 24 hours for your Preferred Name to update.', createdAt=1737745609421, sources=None)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_id = \"9112a4c0-6a2e-4a56-9280-5e25c95f8888\"\n",
    "user_id = \"9811f3a0-d0d1-701b-29e4-f7a680d50e85\"\n",
    "messages, token = list_chat_messages(user_id, chat_id)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = summarizer._create_summarization_prompt(messages)\n",
    "\n",
    "# write prompt to a file\n",
    "with open(\"prompt.txt\", \"w\") as f:\n",
    "    f.write(prompt[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"You are a detailed note-taker for a customer service chatbot that helps users solve technical issues. You carefully read through conversations and focus on the details you're asked to find by the system. Do not output anything except text.\\n\\nCreate a summary of the conversation below that captures the key points of the conversation. Keep the summary to a few bullet points.\\n\\nCONVERSATION START:\\n\\nHuman: hello\\n\\n\\nChatbot: Hello! I'm the Cal Poly IT Help Desk assistant. How can I help you today?\\n\\nHuman: how do I change my display name?\\n\\nChatbot: Here are the steps to change your display name when sending email from Outlook:\\n\\n1. Log in to the My Cal Poly Portal > Personal Info tab.\\n2. Under Personal Info > My Info, by the Preferred Name field click the Edit link.\\n3. Click the Continue link, then:\\n   a. Employees: Name Type > Preferred, click the Edit button.\\n   b. Students: Personal Info > Names, click the Edit button.\\n4. Enter your name using upper and lower case letters. Do not include periods. Click the Save button, then the OK button.\\n\\nNote: It may take up to 24 hours for your Preferred Name to update.\\n\\nCONVERSATION END\\n\\nEspecially focus on the following types of details:\\n- The user's primary issue\\n- Questions that the user asked\\n- Places where the user seemed confused\\n- Places where the user got stuck\\n- Recommendations the AI made\\n- Solutions that did or didn't work\\n- Places where the user seemed frustrated or wanted to talk to a human\\n...as well as any other details that are important to the purpose of the conversation.\"}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = summarizer._create_summarization_prompt(messages)\n",
    "prompt, system_prompts = prompts[\"prompt\"], prompts.get(\"system_prompts\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Summary:\\n\\n- The user's primary issue was how to change their display name when sending email from Outlook.\\n- The chatbot provided step-by-step instructions on how to update the preferred name in the My Cal Poly Portal.\\n- The steps included logging into the portal, navigating to the Personal Info tab, and editing the Preferred Name field.\\n- The chatbot noted that it may take up to 24 hours for the preferred name change to take effect.\\n- The user did not seem confused or frustrated during the conversation, and the provided solution appeared to address their issue.\""
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarizer.summarize(messages)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_summary(summary: str, model_name: str) -> None:\n",
    "    if not os.path.exists(save_summary.dir_name):\n",
    "        os.makedirs(save_summary.dir_name)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    path = os.path.join(save_summary.dir_name, f\"{model_name}-{timestamp}.txt\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "\n",
    "save_summary.dir_name = \"summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary(summary, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
